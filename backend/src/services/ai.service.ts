import { prisma } from "../utils/prisma";
import conversationExampleService from "./conversation-example.service";
import openaiService from "./ai-providers/openai.service";
import anthropicService from "./ai-providers/anthropic.service";
import { AIProvider } from "../types/ai-provider";
import { essentialTools } from "./ai-tools";

/**
 * ============================================
 * CONFIGURA√á√ïES DO CHATBOT - VALORES OTIMIZADOS
 * ============================================
 * Estas configura√ß√µes foram otimizadas para melhor performance
 * de um chatbot profissional de atendimento ao cliente.
 * N√ÉO s√£o configur√°veis pelo cliente final.
 */
const CHATBOT_CONFIG = {
  // ===== JANELA DE CONTEXTO =====
  // N√∫mero m√°ximo de mensagens a buscar do banco
  // Usado como limite inicial antes de aplicar otimiza√ß√µes
  MAX_MESSAGES_TO_FETCH: 20,

  // ===== LIMITE DE TOKENS DO HIST√ìRICO =====
  // M√°ximo de tokens permitidos para o hist√≥rico de mensagens
  // GPT-4o Mini tem 128k de contexto, mas reservamos espa√ßo para:
  // - System prompt (~1500 tokens)
  // - Resposta (~400 tokens)
  // - Margem de seguran√ßa
  MAX_HISTORY_TOKENS: 2000,

  // ===== TEMPERATURA =====
  // Controla criatividade vs consist√™ncia das respostas
  // 0.0 = muito determin√≠stico, sempre mesma resposta
  // 0.3-0.5 = consistente mas com varia√ß√£o natural (IDEAL PARA ATENDIMENTO)
  // 0.7-1.0 = mais criativo, pode variar muito
  TEMPERATURE: 0.4,

  // ===== MAX TOKENS DE RESPOSTA =====
  // Limite m√°ximo de tokens na resposta da IA
  // 300 = respostas curtas e diretas (ideal WhatsApp)
  // 500 = respostas m√©dias com mais detalhes
  // 800 = respostas longas para explica√ß√µes complexas
  MAX_TOKENS: 400,

  // ===== CONFIGURA√á√ïES DE RETRY =====
  // Tentativas em caso de falha na API
  MAX_RETRIES: 2,
  RETRY_DELAY_MS: 1000,

  // ===== MODELO PADR√ÉO =====
  // Modelo usado quando n√£o especificado
  DEFAULT_MODEL: "gpt-4o-mini",

  // ===== PRESEN√áA E FREQU√äNCIA =====
  // Penalidades para evitar repeti√ß√µes
  // 0 = sem penalidade, 2.0 = m√°xima penalidade
  PRESENCE_PENALTY: 0.1,  // Evita repetir t√≥picos j√° mencionados
  FREQUENCY_PENALTY: 0.1, // Evita repetir palavras/frases
};

/**
 * ============================================
 * UTILIT√ÅRIOS DE CONTAGEM DE TOKENS
 * ============================================
 * Estimativa de tokens usando regra pr√°tica:
 * ~4 caracteres = 1 token (para portugu√™s)
 * ~0.75 palavras = 1 token
 */

/**
 * Estima o n√∫mero de tokens em um texto
 * Usa aproxima√ß√£o: 1 token ‚âà 4 caracteres para portugu√™s
 */
function estimateTokens(text: string): number {
  if (!text) return 0;
  // Regra pr√°tica: ~4 caracteres por token em portugu√™s
  // Considera tamb√©m espa√ßos e pontua√ß√£o
  return Math.ceil(text.length / 4);
}

/**
 * Interface para mensagem agrupada
 */
interface GroupedMessage {
  sender: string;
  senderType: string;
  messages: string[];
  hasMedia: boolean;
  mediaTypes: string[];
  tokenCount: number;
}

class AIService {
  /**
   * Obt√©m o provedor de IA configurado
   */
  private getProvider(providerName?: AIProvider) {
    // Usa o provedor especificado ou o padr√£o do .env
    const provider = providerName || (process.env.AI_PROVIDER as AIProvider) || "openai";

    switch (provider) {
      case "openai":
        return openaiService;
      case "anthropic":
        return anthropicService;
      default:
        console.warn(`Unknown AI provider: ${provider}. Falling back to OpenAI.`);
        return openaiService;
    }
  }

  /**
   * Gera resposta autom√°tica usando o provedor configurado
   */
  async generateResponse(
    customerId: string,
    message: string,
    options?: { provider?: AIProvider; model?: string; temperature?: number; maxTokens?: number }
  ): Promise<string> {
    try {
      // Busca o customer com informa√ß√µes da empresa
      const customer = await prisma.customer.findUnique({
        where: { id: customerId },
        include: {
          company: {
            include: {
              aiKnowledge: true,
            },
          },
        },
      });

      if (!customer) {
        throw new Error("Customer not found");
      }

      // üéØ JANELA DE CONTEXTO DESLIZANTE COM OTIMIZA√á√ÉO
      // Busca mais mensagens do que precisamos para ter margem de otimiza√ß√£o
      const messages = await prisma.message.findMany({
        where: { customerId },
        orderBy: { timestamp: "desc" },
        take: CHATBOT_CONFIG.MAX_MESSAGES_TO_FETCH,
        include: {
          customer: true,
        },
      });

      // Inverte para ordem cronol√≥gica (mais antiga primeiro)
      const messageHistory = messages.reverse();

      // Monta o contexto da empresa
      const aiKnowledge = customer.company.aiKnowledge;

      // Verifica se resposta autom√°tica est√° habilitada
      // Por padr√£o, a IA responde EXCETO se autoReplyEnabled === false explicitamente
      if (aiKnowledge && aiKnowledge.autoReplyEnabled === false) {
        throw new Error("Auto-reply is disabled for this company");
      }

      const companyInfo = aiKnowledge?.companyInfo || "Informa√ß√µes da empresa n√£o dispon√≠veis.";
      const productsServices = aiKnowledge?.productsServices || "Produtos/servi√ßos n√£o especificados.";
      const toneInstructions = aiKnowledge?.toneInstructions || "Seja profissional, educado e prestativo.";
      const policies = aiKnowledge?.policies || "Nenhuma pol√≠tica espec√≠fica definida.";
      const negativeExamples = aiKnowledge?.negativeExamples || null;

      // Pega configura√ß√µes avan√ßadas da IA
      // NOTA: temperatura e maxTokens usam valores otimizados fixos (n√£o configur√°veis pelo cliente)
      const providerConfig = aiKnowledge?.provider as AIProvider | undefined;
      const modelConfig = aiKnowledge?.model ?? CHATBOT_CONFIG.DEFAULT_MODEL;
      const temperature = CHATBOT_CONFIG.TEMPERATURE;
      const maxTokens = CHATBOT_CONFIG.MAX_TOKENS;

      // üéØ OTIMIZA√á√ÉO: Agrupa mensagens sequenciais do mesmo remetente
      // e aplica limite de tokens para n√£o estourar contexto
      const { historyText, stats } = this.buildOptimizedHistory(messageHistory, customer.name);

      console.log(`[AIService] Context stats: ${stats.totalMessages} msgs ‚Üí ${stats.groupedBlocks} blocks, ~${stats.totalTokens} tokens`);

      // Busca exemplos de conversas exemplares (limitado para otimiza√ß√£o)
      const examplesText = await conversationExampleService.getExamplesForPrompt(customer.companyId);

      // Monta o prompt otimizado para GPT-4o Mini
      // Prompt mais conciso e estruturado para economizar tokens
      const systemPrompt = this.buildOptimizedPrompt({
        companyName: customer.company.name,
        companyInfo,
        productsServices,
        toneInstructions,
        policies,
        examplesText,
        negativeExamples,
        customerName: customer.name,
        customerPhone: customer.phone,
        customerEmail: customer.email,
        customerTags: customer.tags,
        customerNotes: customer.notes,
      });

      const userPrompt = this.buildUserPrompt(historyText, message);

      // Seleciona e usa o provedor (prioriza configura√ß√£o da empresa)
      const providerName = options?.provider || providerConfig || (process.env.AI_PROVIDER as AIProvider) || "openai";
      const provider = this.getProvider(providerName);

      if (!provider.isConfigured()) {
        throw new Error(`AI provider is not configured. Please check your environment variables.`);
      }

      const lastMessage = messageHistory[messageHistory.length - 1];
      let imageUrlForVision: string | undefined = undefined;

      // Se a √∫ltima mensagem do cliente for uma imagem, passamos para a IA analisar
      if (lastMessage && lastMessage.direction === "INBOUND" && lastMessage.mediaType === "image" && lastMessage.mediaUrl) {
        imageUrlForVision = lastMessage.mediaUrl;
        console.log("[AIService] Image detected, enabling Vision capabilities");
      }

      // üéØ Function Calling: Passa tools apenas para OpenAI (Anthropic n√£o suporta ainda)
      const useTools = providerName === "openai" || (options?.provider || providerConfig) === "openai";

      const aiResponse = await provider.generateResponse({
        systemPrompt,
        userPrompt,
        temperature,
        maxTokens,
        model: options?.model || modelConfig,
        imageUrl: imageUrlForVision,
        // Adiciona tools e contexto para Function Calling
        ...(useTools && {
          tools: essentialTools,
          toolChoice: "auto", // IA decide quando usar
          context: {
            customerId: customer.id,
            companyId: customer.companyId,
          },
        }),
      });

      // Remove qualquer formata√ß√£o Markdown que a IA possa ter usado
      const cleanResponse = this.removeMarkdown(aiResponse);

      return cleanResponse;
    } catch (error: any) {
      console.error("AI Error:", error.message);
      throw new Error(`Failed to generate AI response: ${error.message}`);
    }
  }

  /**
   * Constr√≥i hist√≥rico otimizado com agrupamento e limite de tokens
   *
   * Otimiza√ß√µes:
   * 1. Agrupa mensagens sequenciais do mesmo remetente
   * 2. Aplica limite de tokens para n√£o estourar contexto
   * 3. Prioriza mensagens mais recentes
   */
  private buildOptimizedHistory(
    messageHistory: any[],
    customerName: string
  ): { historyText: string; stats: { totalMessages: number; groupedBlocks: number; totalTokens: number } } {
    if (!messageHistory || messageHistory.length === 0) {
      return {
        historyText: "(In√≠cio da conversa)",
        stats: { totalMessages: 0, groupedBlocks: 0, totalTokens: 0 },
      };
    }

    // 1. Agrupa mensagens sequenciais do mesmo remetente
    const groupedMessages: GroupedMessage[] = [];
    let currentGroup: GroupedMessage | null = null;

    for (const msg of messageHistory) {
      const isInbound = msg.direction === "INBOUND";
      const sender = isInbound ? customerName : "Assistente";
      const senderType = isInbound ? "customer" : (msg.senderType === "HUMAN" ? "human" : "ai");

      // Detecta m√≠dia
      const mediaType = msg.mediaType || null;

      if (currentGroup && currentGroup.sender === sender && currentGroup.senderType === senderType) {
        // Mesma pessoa, adiciona √† mensagem atual
        currentGroup.messages.push(msg.content);
        if (mediaType) {
          currentGroup.hasMedia = true;
          if (!currentGroup.mediaTypes.includes(mediaType)) {
            currentGroup.mediaTypes.push(mediaType);
          }
        }
      } else {
        // Nova pessoa, cria novo grupo
        if (currentGroup) {
          groupedMessages.push(currentGroup);
        }
        currentGroup = {
          sender,
          senderType,
          messages: [msg.content],
          hasMedia: !!mediaType,
          mediaTypes: mediaType ? [mediaType] : [],
          tokenCount: 0,
        };
      }
    }

    // Adiciona √∫ltimo grupo
    if (currentGroup) {
      groupedMessages.push(currentGroup);
    }

    // 2. Calcula tokens e formata cada bloco
    const formattedBlocks: string[] = [];
    let totalTokens = 0;

    // Processa do mais recente para o mais antigo (para priorizar recentes)
    const reversedGroups = [...groupedMessages].reverse();

    for (const group of reversedGroups) {
      // Formata o bloco
      let senderLabel = group.sender;
      if (group.senderType === "human") {
        senderLabel += " (Atendente)";
      }

      // Indicador de m√≠dia
      let mediaIndicator = "";
      if (group.hasMedia) {
        if (group.mediaTypes.includes("audio")) mediaIndicator += " üé§";
        if (group.mediaTypes.includes("image")) mediaIndicator += " üì∑";
      }

      // Une mensagens do mesmo remetente com quebra de linha simples
      const content = group.messages.join("\n");
      const blockText = `${senderLabel}${mediaIndicator}: ${content}`;

      // Calcula tokens do bloco
      const blockTokens = estimateTokens(blockText);

      // Verifica se ainda cabe no limite
      if (totalTokens + blockTokens > CHATBOT_CONFIG.MAX_HISTORY_TOKENS) {
        // N√£o cabe mais, para de adicionar
        console.log(`[AIService] Token limit reached (${totalTokens}/${CHATBOT_CONFIG.MAX_HISTORY_TOKENS}), stopping at ${formattedBlocks.length} blocks`);
        break;
      }

      formattedBlocks.unshift(blockText); // Adiciona no in√≠cio para manter ordem cronol√≥gica
      totalTokens += blockTokens;
      group.tokenCount = blockTokens;
    }

    // 3. Junta todos os blocos
    const historyText = formattedBlocks.join("\n\n");

    return {
      historyText,
      stats: {
        totalMessages: messageHistory.length,
        groupedBlocks: formattedBlocks.length,
        totalTokens,
      },
    };
  }

  /**
   * Remove formata√ß√£o Markdown da resposta da IA
   * WhatsApp n√£o renderiza markdown, ent√£o removemos para evitar ** e _ aparecendo no texto
   */
  private removeMarkdown(text: string): string {
    return (
      text
        // Remove bold: **texto** ou __texto__
        .replace(/\*\*(.+?)\*\*/g, "$1")
        .replace(/__(.+?)__/g, "$1")
        // Remove italic: *texto* ou _texto_
        .replace(/\*(.+?)\*/g, "$1")
        .replace(/_(.+?)_/g, "$1")
        // Remove strikethrough: ~~texto~~
        .replace(/~~(.+?)~~/g, "$1")
        // Remove code: `texto`
        .replace(/`(.+?)`/g, "$1")
        // Remove headers: # texto
        .replace(/^#+\s+/gm, "")
        // Remove listas: - item ou * item
        .replace(/^[\*\-]\s+/gm, "")
        // Remove links: [texto](url)
        .replace(/\[(.+?)\]\(.+?\)/g, "$1")
        // Remove > (quote)
        .replace(/^>\s+/gm, "")
    );
  }

  /**
   * Constr√≥i prompt otimizado para chatbot profissional
   * Gen√©rico para qualquer tipo de empresa/segmento
   */
  private buildOptimizedPrompt(data: any): string {
    const { companyName, companyInfo, productsServices, toneInstructions, policies, negativeExamples, customerName } = data;

    return `VOC√ä √â: Assistente Virtual da ${companyName}
FUN√á√ÉO: Atendimento ao cliente via WhatsApp

# INFORMA√á√ïES DA EMPRESA
${companyInfo || "Empresa de atendimento ao cliente."}

# PRODUTOS E SERVI√áOS
${productsServices || "Consulte o atendente para informa√ß√µes sobre produtos e servi√ßos."}

# POL√çTICAS E REGRAS
${policies || ""}

# TOM DE VOZ E COMPORTAMENTO
${toneInstructions || "Seja profissional, educado e prestativo. Use linguagem clara e objetiva."}

# üîí REGRAS DE SEGURAN√áA (CR√çTICO - NUNCA VIOLE)

**INFORMA√á√ïES PROIBIDAS - NUNCA REVELE:**
- Dados financeiros da empresa (faturamento, lucro, custos)
- Dados pessoais de funcion√°rios ou outros clientes
- Senhas, acessos, credenciais ou informa√ß√µes t√©cnicas internas
- Estrat√©gias de neg√≥cio ou informa√ß√µes confidenciais

**ASSUNTOS PROIBIDOS - NUNCA DISCUTA:**
- Pol√≠tica, religi√£o ou temas pol√™micos
- Opini√µes pessoais sobre qualquer assunto
- Compara√ß√µes negativas com concorrentes
- Fofocas ou assuntos n√£o relacionados ao neg√≥cio

**AO RECEBER PERGUNTA PROIBIDA, RESPONDA:**
"Desculpe, n√£o posso ajudar com esse assunto. üîí Posso te ajudar com informa√ß√µes sobre nossos produtos, servi√ßos ou agendamentos. Como posso te auxiliar?"

**Se cliente insistir 2+ vezes em assuntos proibidos ‚Üí use [TRANSBORDO]**

${negativeExamples ? `
# ‚ùå O QUE N√ÉO FAZER (Configurado pela empresa)
${negativeExamples}
` : ""}

# üìã DIRETRIZES DE ATENDIMENTO

1. **Comunica√ß√£o:**
   - Respostas curtas e objetivas (m√°ximo 3-4 linhas)
   - Linguagem clara, sem jarg√µes t√©cnicos desnecess√°rios
   - Emojis com modera√ß√£o e apenas quando apropriado
   - N√ÉO use formata√ß√£o Markdown (*, **, _, etc.)
   - Se j√° houver hist√≥rico, N√ÉO repita sauda√ß√µes

2. **√Åudios do Cliente:**
   - O sistema transcreveu automaticamente
   - Responda naturalmente SEM mencionar que era √°udio
   - Trate como mensagem de texto normal

3. **Imagens do Cliente:**
   - Analise o conte√∫do relevante da imagem
   - Comente de forma √∫til sobre o que foi enviado
   - Use a an√°lise para ajudar melhor o cliente

4. **Qualifica√ß√£o:**
   - Entenda a necessidade antes de oferecer solu√ß√µes
   - Fa√ßa 1-2 perguntas por vez, n√£o sobrecarregue
   - Personalize a resposta com base no contexto

5. **Fechamento:**
   - Termine com UMA pergunta de a√ß√£o clara
   - Evite m√∫ltiplas perguntas que confundem
   - Direcione para o pr√≥ximo passo

# üìÖ AGENDAMENTOS

Use [INICIAR_AGENDAMENTO] no IN√çCIO da resposta APENAS quando:
‚úÖ Cliente usa: "quero agendar", "preciso marcar", "tem hor√°rio?"
‚úÖ Decis√£o clara: "vou agendar", "pode marcar"

N√ÉO use quando:
‚ùå Apenas perguntando sobre servi√ßos
‚ùå Pedindo pre√ßos ou informa√ß√µes
‚ùå Indeciso ou explorando op√ß√µes

Formato: [INICIAR_AGENDAMENTO] Sua mensagem aqui...

# üö® TRANSBORDO PARA HUMANO

Use [TRANSBORDO] no IN√çCIO da resposta quando:
‚úÖ Cliente pede: "quero falar com atendente/humano"
‚úÖ Reclama√ß√µes graves ou cliente muito insatisfeito
‚úÖ Problemas com pagamentos, garantia ou devolu√ß√£o
‚úÖ Negocia√ß√µes especiais ou projetos complexos
‚úÖ Situa√ß√µes que voc√™ n√£o consegue resolver
‚úÖ Cliente insiste em assuntos proibidos (2+ vezes)

N√ÉO transfira para:
‚ùå D√∫vidas simples sobre produtos/servi√ßos
‚ùå Pedidos de or√ßamento padr√£o
‚ùå Agendamentos normais

Formato: [TRANSBORDO] Vou transferir voc√™ para um especialista que pode ajudar melhor. Um momento!

# üë§ DADOS DO CLIENTE
Nome: ${customerName}
${data.customerTags?.length ? `Tags: ${data.customerTags.join(", ")}` : ""}
${data.customerNotes ? `Observa√ß√µes: ${data.customerNotes}` : ""}

Responda de forma natural e conversacional:`;
  }

  /**
   * Constr√≥i prompt do usu√°rio
   */
  private buildUserPrompt(historyText: string, currentMessage: string): string {
    // Adiciona data/hora atual para no√ß√£o temporal
    const now = new Date().toLocaleString("pt-BR", { timeZone: "America/Sao_Paulo" });

    return `DATA/HORA ATUAL: ${now}

# HIST√ìRICO DA CONVERSA (Mensagens Anteriores)
${historyText ? historyText : "(In√≠cio da conversa)"}

# MENSAGEM ATUAL DO CLIENTE
${currentMessage}

Sua resposta (lembre-se: sem 'Oi' repetitivo se j√° houver hist√≥rico):`;
  }

  /**
   * Verifica se algum provedor est√° configurado
   */
  isConfigured(): boolean {
    return openaiService.isConfigured() || anthropicService.isConfigured();
  }

  /**
   * Retorna informa√ß√µes sobre o provedor atual
   */
  getCurrentProviderInfo() {
    const providerName = (process.env.AI_PROVIDER as AIProvider) || "openai";
    const provider = this.getProvider(providerName);
    return provider.getModelInfo();
  }

  /**
   * Lista todos os provedores dispon√≠veis
   */
  getAvailableProviders() {
    return [
      {
        name: "openai",
        configured: openaiService.isConfigured(),
        info: openaiService.getModelInfo(),
      },
      {
        name: "anthropic",
        configured: anthropicService.isConfigured(),
        info: anthropicService.getModelInfo(),
      },
    ];
  }
}

export default new AIService();
